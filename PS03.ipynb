{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH-lYYjqHwK6"
      },
      "source": [
        "Diego Toribio <br>\n",
        "Professor Fred Fontaine <br>\n",
        "EID-378 Finance <br>\n",
        "Problem Set III: MPT and Risk<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-03-08T20:27:26.622048Z",
          "iopub.status.busy": "2025-03-08T20:27:26.621706Z",
          "iopub.status.idle": "2025-03-08T20:27:31.584165Z",
          "shell.execute_reply": "2025-03-08T20:27:31.582547Z",
          "shell.execute_reply.started": "2025-03-08T20:27:26.622025Z"
        },
        "id": "NZyBwNjSHwK-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install yfinance --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EfDrEcVoKJ4L"
      },
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si79LMLnHwLA"
      },
      "source": [
        "In this assignment, you will be exploring:\n",
        "\n",
        "- Modern Portfolio Theory, including concepts such as the \\( \\beta \\) and Sharpe ratio.\n",
        "- V@R and CV@R risk measures, including their application to portfolio optimization (as an alternative to MPT)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQlHzWWCHwLA"
      },
      "source": [
        "## Section 1 - Preparing the Data\n",
        "\n",
        "1. Select **M = 5** stocks, and get the daily adjusted closing price over one year for them.\n",
        "\n",
        "2. Also get the S&P 500 index (daily) over the same year.\n",
        "\n",
        "3. We also want to determine a risk-free return to use for our portfolio models, based on 3-Month Treasury Bills. This will be explained separately.\n",
        "\n",
        "4. Compute the daily returns of the stocks and S&P 500 over the year. In order to have a return for each trading day, you should also grab the last trading day of the previous year. For example, for 2014, you should also grab Dec. 31, 2013, in order to compute the return for the first trading day (Jan. 2) of 2014. This idea is called 'pre-windowing.' It is fine if you determine the last trading day of the previous year \"manually\" (i.e., rather than writing systematic code that would \"always\" work).\n",
        "\n",
        "5. Subtract off the daily risk-free return from the daily returns of the stocks and S&P 500. Going forward, we are going to be working with these excess returns only (I will omit using the word \"excess\" but keep in mind, everything is excess!). For example, if we think in terms of the Markowitz bullet in the $ (\\sigma, \\mu) $ plane, the 'risk-free' point in the graph will actually be the origin (0,0). More generally, going forward in the analysis, you can link what is happening here back to the \"theory\" discussed in the lecture notes by acting as if the risk-free return is 0!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxXZj6r_HwLB"
      },
      "source": [
        "#### 1.1 – Adjusted Close Data Retrieval  \n",
        "We include the last trading day of the prior year so that our first return can be computed on the first trading day of the target year. Here we download auto‑adjusted close prices for our $M=5$ stocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2025-03-08T20:19:39.323779Z",
          "iopub.status.busy": "2025-03-08T20:19:39.323335Z",
          "iopub.status.idle": "2025-03-08T20:19:39.367212Z",
          "shell.execute_reply": "2025-03-08T20:19:39.366085Z",
          "shell.execute_reply.started": "2025-03-08T20:19:39.323741Z"
        },
        "id": "BJcXblseHwLC",
        "outputId": "d2bd41f3-6d1c-40df-f830-bd3bef4adf7b",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1mTable 1\u001b[0m. Adjusted Close Prices for Selected Stocks (first five rows)\n",
            "+------------+--------+--------+--------+--------+--------+\n",
            "|    Date    |  AAPL  |  AMZN  |  GOOG  |  MSFT  |  TSLA  |\n",
            "+------------+--------+--------+--------+--------+--------+\n",
            "| 2021-12-31 | 174.52 | 166.72 | 144.00 | 327.16 | 352.26 |\n",
            "| 2022-01-03 | 178.88 | 170.40 | 144.39 | 325.63 | 399.93 |\n",
            "| 2022-01-04 | 176.61 | 167.52 | 143.74 | 320.05 | 383.20 |\n",
            "| 2022-01-05 | 171.91 | 164.36 | 137.00 | 307.76 | 362.71 |\n",
            "| 2022-01-06 | 169.04 | 163.25 | 136.90 | 305.33 | 354.90 |\n",
            "+------------+--------+--------+--------+--------+--------+\n"
          ]
        }
      ],
      "source": [
        "# download adjusted close prices\n",
        "start_date = \"2021-12-31\"\n",
        "end_date   = \"2022-12-31\"\n",
        "tickers     = [\"AAPL\", \"MSFT\", \"GOOG\", \"AMZN\", \"TSLA\"]\n",
        "stock_data  = yf.download(\n",
        "    tickers,\n",
        "    start=start_date,\n",
        "    end=end_date,\n",
        "    progress=False,\n",
        "    auto_adjust=True\n",
        ")[\"Close\"]\n",
        "\n",
        "# prepare first five rows for display\n",
        "df_head = stock_data.head()\n",
        "rows    = [\n",
        "    [date.strftime(\"%Y-%m-%d\")] + [f\"{v:.2f}\" for v in row]\n",
        "    for date, row in df_head.iterrows()\n",
        "]\n",
        "\n",
        "print(\"\\n\\033[1mTable 1\\033[0m. Adjusted Close Prices for Selected Stocks (first five rows)\")\n",
        "print(tabulate(rows, headers=[\"Date\"] + list(df_head.columns), tablefmt=\"pretty\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tru3gH47HwLH"
      },
      "source": [
        "## Section 2 - Data Analysis\n",
        "###  Modern Portfolio Theory\n",
        "\n",
        "#### Some Equations\n",
        "\n",
        "Let us write some equations (which apply in this form—with the risk-free return already subtracted off). Let $ C $ denote the covariance of the returns of the risky stocks (not the S&P 500), $ \\mathbf{m} $ the vector of their expected (excess) returns. Given a portfolio weight vector $ \\mathbf{w} $ (satisfying $\\sum w_k = 1$), the expected (excess) return and standard deviation for $ \\mathbf{w} $ is:\n",
        "\n",
        "$$\n",
        "\\mu(\\mathbf{w}) = \\mathbf{m}^T\\mathbf{w} \\\\\n",
        "\\sigma(\\mathbf{w}) = \\sqrt{\\mathbf{w}^T C \\mathbf{w}}\n",
        "$$\n",
        "\n",
        "Also let $ \\mathbf{1} $ denote the vector of all ones, as usual.  \n",
        "The weight vectors for the MVP and MP are:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}_{MVP} = \\frac{1}{\\mathbf{1}^T C^{-1}\\mathbf{1}} C^{-1}\\mathbf{1} \\\\\n",
        "\\mathbf{w}_{MP} = \\frac{1}{\\mathbf{1}^T C^{-1}\\mathbf{m}} C^{-1}\\mathbf{m}\n",
        "$$\n",
        "\n",
        "From these you can compute the $\\mu, \\sigma$.\n",
        "\n",
        "If you have two portfolios (of the risky stocks only) with weight vectors $ \\mathbf{w}_1, \\mathbf{w}_2 $, the corresponding returns $ r_1, r_2 $ have covariance given by:\n",
        "\n",
        "$$\n",
        "cov(r_1, r_2) = \\mathbf{w}_1^T C \\mathbf{w}_2\n",
        "$$\n",
        "\n",
        "Otherwise, if you have the actual set of returns over a period of time, you can compute the covariance directly from the data. For example, you can do this to compute the covariance between one of your stocks and the S&P 500. If you store the covariances between each stock and the S&P 500 in a vector $ \\mathbf{q} $, then the covariance between a portfolio $ \\mathbf{w} $ of the stocks and the S&P 500 would be:\n",
        "\n",
        "$$\n",
        "cov(r_{\\mathbf{w}}, r_{S\\&P500}) = \\mathbf{q}^T\\mathbf{w}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "1. Using the data for the full year, we want to get the vector of expected returns and covariance matrix of the returns of the **M = 5** stocks, and the covariance between each stock and the S&P 500. Compute $ \\mathbf{w}_{MVP}, (\\sigma_{MVP}, \\mu_{MVP}) $ and $ \\mathbf{w}_{MP}, (\\sigma_{MP}, \\mu_{MP}) $ for the minimum variance and market portfolios, respectively. We will also need code that, given a weight vector for a portfolio from the stocks $ \\mathbf{w} $, we can compute the $ (\\sigma,\\mu) $ point; also that we can compute the covariance between two such portfolios; we also want the vector $ \\mathbf{q} $ of covariances with the S&P 500, and the ability to compute the covariance of a portfolio given by $ \\mathbf{w} $ with the S&P 500.\n",
        "\n",
        "2. Look at the MVP and MP weight vectors. Do they involve short-selling?\n",
        "\n",
        "3. Compute the Sharpe ratio and $ \\beta $ for each of the **M = 5** stocks, the S&P 500, and the MVP. Compute the covariance between the individual stocks, and the MVP and MP portfolios with the S&P 500. Display these results in a reasonable form, such as a table. Also provide a quick summary of which of these are positively or negatively correlated with the S&P 500.\n",
        "\n",
        "4. Generate **N = 100** random $ \\mathbf{w} $ vectors with **no short selling**; they have to satisfy the constraint that $\\sum_{k=1}^{M} w_k = 1$. To do this I suggest: (1) generate $ M $ independent numbers uniformly from 0 to 1, say $(x_1, x_2, \\dots, x_5)$, and then (2) set $ w_k = x_k / (\\sum x_k) $, i.e., normalize them to sum to 1. I suggest you store these vectors in an array (dataframe) as you will be using them again later!\n",
        "\n",
        "5. Compute the $ (\\sigma, \\mu) $ point for each of these vectors, and create a scatter plot of these $ (\\sigma, \\mu) $, also put in $ (\\sigma_{MVP}, \\mu_{MVP}) $, and $ (\\sigma_{MP}, \\mu_{MP}) $ and also compute and plot $ (\\sigma_{SP}, \\mu_{SP}) $ for the S&P 500. Also draw the CAL on your graph (remember, the \"risk-free\" point is now (0,0)).\n",
        "\n",
        "6. Find the median of the expected returns of those 100 portfolios that involve no short-selling, say $ \\mu_{med} $. Find the \"best\" portfolio (among those you created) whose return satisfies $ \\mu \\geq \\mu_{med} $, i.e., among all such, the $ \\sigma $ is minimum. Highlight that portfolio. We can consider that portfolio as an approximate solution to an optimization problem (find the portfolio with the no short selling constraint and achieves at least a prescribed expected return that minimizes risk *as measured by standard deviation*). For that optimal portfolio, compute its Sharpe ratio and $ \\beta $.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbitOshPHwLL"
      },
      "source": [
        "## Section 3 - V@R and CV@R\n",
        "\n",
        "1. Use the same dataset you collected for the previous problem. Consider the returns over one year for each of your stocks, plus S&P 500.\n",
        "\n",
        "  (a) Construct histograms for the returns with 50 bins, and plot them, for each of your stocks.\n",
        "\n",
        "  (b) Compute the V@R$_{1-\\alpha}$ and CV@R$_{1-\\alpha}$ for $\\alpha = 5\\%$ and $10\\%$, directly from the daily returns over the year, for each stock separately. **Note:** In practice, 5% and 1% are more common, but given the limited number of data points available, we are not likely to obtain reasonable results for 1%.\n",
        "\n",
        "2. Again, we are using the same dataset.\n",
        "\n",
        "  (a) Given the set of daily returns for the five stocks over one year, and a weight vector $ \\mathbf{w} $ for a portfolio, write code to compute the V@R and CV@R for prescribed $\\alpha$.\n",
        "\n",
        "  (b) Compute the V@R$_{1-\\alpha}$ and CV@R$_{1-\\alpha}$ for the MVP, MP, and S&P 500.\n",
        "\n",
        "  (c) Use the **SAME** set of 100 random weight vectors with no short selling as you used from the MPT section of this assignment. You already have the expected returns. Now compute the CV@R$_{1-\\alpha}$ for $\\alpha = 10\\%$ and $\\alpha = 5\\%$ for each of them.\n",
        "\n",
        "  (d) Obtain a scatter plot with axes $(CV@R_{1-\\alpha}, \\mu)$ for your 100 weight vectors, plus the MVP, MP, S&P 500, and each of your individual stocks, for $\\alpha = 10\\%$. Obtain a separate scatter plot for $\\alpha = 5\\%$.\n",
        "\n",
        "\n",
        "  (f) You now have three (estimated) \"optimal\" portfolios obtained under 3 different risk measures: standard deviation, and CV@R at two levels. Summarize in a table for these 3 the following: the expected return, Sharpe ratio, $\\beta$, standard deviation, and both CV@R values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJIjVIktHwLM"
      },
      "source": [
        "## Section 4 - Comment\n",
        "\n",
        "You obtained three scatter plots corresponding to three criteria for optimization. The \"upper bound\" on your scatter plots represent, essentially, \"efficient frontiers\" with respect to these risk measures (i.e., for a given return, the risk measure is minimized). Recall that for the standard deviation, this \"upper bound\" is a hyperbola, and specifically is a concave curve. Do your results appear to be concave in each of the other cases?\n",
        "\n",
        "Here, I asked you to create 100 random portfolios. You may want to try more, if your computer can handle it. If you work with 1000, do your scatter plots and final \"optimal\" choices change considerably?\n",
        "\n",
        "One more comment: in many cases, the V@R and CV@R are computed using the formulas that apply for the case of a Gaussian model. This may be useful in achieving a more systematic approach to solving the optimization problem, rather than the crude \"random search\" given here, but again presumes the Gaussian model is accurate."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
